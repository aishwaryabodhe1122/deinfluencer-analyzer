"""
Universal Social Media Data Fetcher
Fetches real-time data for ANY influencer from ANY platform using multiple methods
"""

import os
import re
import json
import asyncio
import aiohttp
from typing import Dict, Optional, List
from datetime import datetime, timedelta
import hashlib
from bs4 import BeautifulSoup
import time

class UniversalSocialFetcher:
    """
    Universal fetcher that can get real-time data for any influencer on any platform
    """
    
    def __init__(self):
        self.session = None
        self.cache = {}  # Simple in-memory cache
        self.cache_duration = 300  # 5 minutes cache
        
        # API Keys from environment variables
        self.youtube_api_key = os.getenv('YOUTUBE_API_KEY')
        self.twitter_bearer_token = os.getenv('TWITTER_BEARER_TOKEN')
        self.instagram_access_token = os.getenv('INSTAGRAM_ACCESS_TOKEN')
        
    async def get_session(self):
        """Get or create aiohttp session"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10),
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
            )
        return self.session
    
    def _get_cache_key(self, username: str, platform: str) -> str:
        """Generate cache key for username and platform"""
        return hashlib.md5(f"{username}_{platform}".encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict) -> bool:
        """Check if cache entry is still valid"""
        if not cache_entry:
            return False
        
        cache_time = cache_entry.get('timestamp', 0)
        return (time.time() - cache_time) < self.cache_duration
    
    async def fetch_any_influencer(self, username: str, platform: str) -> Optional[Dict]:
        """
        Universal method to fetch real-time data for any influencer
        """
        print(f"üîç Fetching real-time data for {username} on {platform}")
        
        # Check cache first
        cache_key = self._get_cache_key(username, platform)
        if cache_key in self.cache and self._is_cache_valid(self.cache[cache_key]):
            print(f"üìã Using cached data for {username}")
            return self.cache[cache_key]['data']
        
        # Try different methods in order of reliability
        methods = [
            self._fetch_via_official_api,
            self._fetch_via_web_scraping,
            self._fetch_via_public_endpoints
        ]
        
        for method in methods:
            try:
                data = await method(username, platform)
                if data and data.get('follower_count', 0) > 0:
                    # Cache the result
                    self.cache[cache_key] = {
                        'data': data,
                        'timestamp': time.time()
                    }
                    print(f"‚úÖ Successfully fetched real-time data for {username}: {data['follower_count']:,} followers")
                    return data
            except Exception as e:
                print(f"‚ö†Ô∏è Method {method.__name__} failed for {username}: {str(e)}")
                continue
        
        print(f"‚ùå All methods failed for {username} on {platform}")
        return None
    
    async def _fetch_via_official_api(self, username: str, platform: str) -> Optional[Dict]:
        """Fetch data using official social media APIs"""
        
        if platform.lower() == 'youtube':
            return await self._fetch_youtube_api(username)
        elif platform.lower() in ['twitter', 'x']:
            return await self._fetch_twitter_api(username)
        elif platform.lower() == 'instagram':
            return await self._fetch_instagram_api(username)
        
        return None
    
    async def _fetch_youtube_api(self, username: str) -> Optional[Dict]:
        """Fetch YouTube data using YouTube Data API v3"""
        if not self.youtube_api_key:
            print("‚ö†Ô∏è YouTube API key not configured")
            return None
        
        session = await self.get_session()
        
        # Try to get channel by username first
        search_url = f"https://www.googleapis.com/youtube/v3/search"
        params = {
            'part': 'snippet',
            'q': username,
            'type': 'channel',
            'maxResults': 1,
            'key': self.youtube_api_key
        }
        
        try:
            async with session.get(search_url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('items'):
                        channel_id = data['items'][0]['snippet']['channelId']
                        return await self._fetch_youtube_channel_stats(channel_id)
        except Exception as e:
            print(f"YouTube API search failed: {e}")
        
        return None
    
    async def _fetch_youtube_channel_stats(self, channel_id: str) -> Optional[Dict]:
        """Fetch YouTube channel statistics"""
        session = await self.get_session()
        
        stats_url = f"https://www.googleapis.com/youtube/v3/channels"
        params = {
            'part': 'statistics,snippet',
            'id': channel_id,
            'key': self.youtube_api_key
        }
        
        try:
            async with session.get(stats_url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('items'):
                        item = data['items'][0]
                        stats = item.get('statistics', {})
                        snippet = item.get('snippet', {})
                        
                        return {
                            'username': snippet.get('title', ''),
                            'platform': 'youtube',
                            'follower_count': int(stats.get('subscriberCount', 0)),
                            'following_count': 0,
                            'post_count': int(stats.get('videoCount', 0)),
                            'bio': snippet.get('description', '')[:200],
                            'verified': True,  # Most channels with API access are verified
                            'engagement_rate': 5.0,
                            'avg_views': int(stats.get('viewCount', 0)) // max(int(stats.get('videoCount', 1)), 1)
                        }
        except Exception as e:
            print(f"YouTube channel stats failed: {e}")
        
        return None
    
    async def _fetch_twitter_api(self, username: str) -> Optional[Dict]:
        """Fetch Twitter data using Twitter API v2"""
        if not self.twitter_bearer_token:
            print("‚ö†Ô∏è Twitter Bearer Token not configured")
            return None
        
        session = await self.get_session()
        
        # Clean username
        clean_username = username.replace('@', '')
        
        url = f"https://api.twitter.com/2/users/by/username/{clean_username}"
        params = {
            'user.fields': 'public_metrics,description,verified'
        }
        headers = {
            'Authorization': f'Bearer {self.twitter_bearer_token}'
        }
        
        try:
            async with session.get(url, params=params, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('data'):
                        user_data = data['data']
                        metrics = user_data.get('public_metrics', {})
                        
                        return {
                            'username': user_data.get('username', ''),
                            'platform': 'twitter',
                            'follower_count': metrics.get('followers_count', 0),
                            'following_count': metrics.get('following_count', 0),
                            'post_count': metrics.get('tweet_count', 0),
                            'bio': user_data.get('description', ''),
                            'verified': user_data.get('verified', False),
                            'engagement_rate': 2.5,
                            'avg_likes': metrics.get('followers_count', 0) * 0.02
                        }
                else:
                    print(f"Twitter API error: {response.status}")
        except Exception as e:
            print(f"Twitter API failed: {e}")
        
        return None
    
    async def _fetch_instagram_api(self, username: str) -> Optional[Dict]:
        """Fetch Instagram data using Instagram Basic Display API"""
        # Instagram Basic Display API is very limited and requires user authorization
        # For now, we'll rely on web scraping for Instagram
        print("‚ö†Ô∏è Instagram API requires user authorization, using web scraping")
        return None
    
    async def _fetch_via_web_scraping(self, username: str, platform: str) -> Optional[Dict]:
        """Fetch data via web scraping public profiles"""
        
        if platform.lower() == 'instagram':
            return await self._scrape_instagram(username)
        elif platform.lower() in ['twitter', 'x']:
            return await self._scrape_twitter(username)
        elif platform.lower() == 'youtube':
            return await self._scrape_youtube(username)
        
        return None
    
    async def _scrape_instagram(self, username: str) -> Optional[Dict]:
        """Scrape Instagram public profile"""
        session = await self.get_session()
        
        # Clean username
        clean_username = username.replace('@', '')
        url = f"https://www.instagram.com/{clean_username}/"
        
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Look for JSON data in script tags
                    scripts = soup.find_all('script', type='application/ld+json')
                    for script in scripts:
                        try:
                            data = json.loads(script.string)
                            if isinstance(data, dict) and 'interactionStatistic' in data:
                                followers = 0
                                for stat in data['interactionStatistic']:
                                    if stat.get('interactionType') == 'http://schema.org/FollowAction':
                                        followers = stat.get('userInteractionCount', 0)
                                        break
                                
                                return {
                                    'username': clean_username,
                                    'platform': 'instagram',
                                    'follower_count': int(followers),
                                    'following_count': 0,  # Not easily scrapable
                                    'post_count': 0,  # Not easily scrapable
                                    'bio': data.get('description', ''),
                                    'verified': True,  # Assume verified if we found structured data
                                    'engagement_rate': 3.5
                                }
                        except (json.JSONDecodeError, KeyError):
                            continue
                    
                    # Fallback: look for meta tags
                    follower_meta = soup.find('meta', property='og:description')
                    if follower_meta:
                        description = follower_meta.get('content', '')
                        # Try to extract follower count from description
                        follower_match = re.search(r'([\d,]+)\s+Followers', description)
                        if follower_match:
                            follower_str = follower_match.group(1).replace(',', '')
                            return {
                                'username': clean_username,
                                'platform': 'instagram',
                                'follower_count': int(follower_str),
                                'following_count': 0,
                                'post_count': 0,
                                'bio': description[:200],
                                'verified': True,
                                'engagement_rate': 3.5
                            }
        
        except Exception as e:
            print(f"Instagram scraping failed: {e}")
        
        return None
    
    async def _scrape_twitter(self, username: str) -> Optional[Dict]:
        """Scrape Twitter public profile (limited due to login requirements)"""
        # Twitter now requires login for most data, making scraping very difficult
        # We'll try basic public endpoints but expect limited success
        print("‚ö†Ô∏è Twitter scraping is limited due to login requirements")
        return None
    
    async def _scrape_youtube(self, username: str) -> Optional[Dict]:
        """Scrape YouTube channel page"""
        session = await self.get_session()
        
        # Try different URL formats
        urls = [
            f"https://www.youtube.com/@{username}",
            f"https://www.youtube.com/c/{username}",
            f"https://www.youtube.com/user/{username}"
        ]
        
        for url in urls:
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # Look for subscriber count in the HTML
                        subscriber_match = re.search(r'"subscriberCountText":\{"simpleText":"([\d.,KMB]+)\s+subscribers"', html)
                        if subscriber_match:
                            sub_text = subscriber_match.group(1)
                            subscriber_count = self._parse_count_string(sub_text)
                            
                            # Look for video count
                            video_match = re.search(r'"videosCountText":\{"runs":\[\{"text":"([\d,]+)"', html)
                            video_count = 0
                            if video_match:
                                video_count = int(video_match.group(1).replace(',', ''))
                            
                            return {
                                'username': username,
                                'platform': 'youtube',
                                'follower_count': subscriber_count,
                                'following_count': 0,
                                'post_count': video_count,
                                'bio': f"YouTube channel: {username}",
                                'verified': subscriber_count > 100000,
                                'engagement_rate': 5.0
                            }
            except Exception as e:
                print(f"YouTube scraping failed for {url}: {e}")
                continue
        
        return None
    
    def _parse_count_string(self, count_str: str) -> int:
        """Parse count strings like '1.2M', '500K', '1,234' to integers"""
        if not count_str:
            return 0
        
        # Remove commas and convert to lowercase
        count_str = count_str.replace(',', '').lower()
        
        # Handle K, M, B suffixes
        multipliers = {'k': 1000, 'm': 1000000, 'b': 1000000000}
        
        for suffix, multiplier in multipliers.items():
            if count_str.endswith(suffix):
                number_part = count_str[:-1]
                try:
                    return int(float(number_part) * multiplier)
                except ValueError:
                    return 0
        
        # Try to parse as regular number
        try:
            return int(count_str)
        except ValueError:
            return 0
    
    async def _fetch_via_public_endpoints(self, username: str, platform: str) -> Optional[Dict]:
        """Fetch data via public endpoints and third-party APIs"""
        # This could include services like Social Blade, etc.
        # For now, we'll implement a basic version
        print(f"‚ö†Ô∏è Public endpoints not yet implemented for {platform}")
        return None
    
    async def close(self):
        """Close the aiohttp session"""
        if self.session:
            await self.session.close()

# Global instance
universal_fetcher = UniversalSocialFetcher()
